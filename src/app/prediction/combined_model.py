# -*- coding: utf-8 -*-
"""combined_model.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1lLL3hJRp9bx8eOJ1HZ11x_VO8xmThQJv
"""


# Commented out IPython magic to ensure Python compatibility.
# from google.colab import drive
# drive.mount('/content/gdrive/')
# %cd /content/gdrive/MyDrive/2025_GDG_Solutions/model
# %ls

import pandas as pd
import numpy as np
import tensorflow as tf
import tensorflow_decision_forests as tfdf
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns
from datetime import datetime, timedelta
import requests
import os
from data_loader import load_merged_data, load_events_data, process_merged_data, prepare_modeling_data

############## PART 1: LOAD FROM data_loader.py ##############
def load_tennessee_data():
    """
    Load Tennessee outage data using functions from data_loader.py
    Returns dictionary with all relevant dataframes
    """
    # Load data
    merged_df = load_merged_data()
    events_df = load_events_data()

    # Process data
    processed_df = process_merged_data(merged_df)
    model_df = prepare_modeling_data(merged_df, events_df)

    return {
        'merged_df': merged_df,
        'events_df': events_df,
        'processed_df': processed_df,
        'model_df': model_df
    }

# ############## PART 2: NOAA Weather API Functions ##############
def fetch_noaa_weather(lat, lng, start_date=None, end_date=None, max_retries=3, timeout=30):
    """
    Fetch weather data from NOAA API for a specific location with improved error handling

    Args:
        lat (float): Latitude
        lng (float): Longitude
        start_date (str, optional): Start date in YYYY-MM-DD format
        end_date (str, optional): End date in YYYY-MM-DD format
        max_retries (int): Maximum number of retry attempts
        timeout (int): Request timeout in seconds

    Returns:
        dict: Weather data from NOAA API
    """
    import requests
    import time

    headers = {
        'User-Agent': '(gemicast-project, fardeen.e.bablu@vanderbilt.edu)',
        'Accept': 'application/geo+json',
    }

    point_url = f"https://api.weather.gov/points/{lat},{lng}"

    # Initialize retry counter
    retries = 0

    while retries < max_retries:
        try:
            # Send request with timeout
            response = requests.get(point_url, headers=headers, timeout=timeout)
            response.raise_for_status()
            point_data = response.json()

            # Extract grid info
            grid_id = point_data['properties']['gridId']
            grid_x = point_data['properties']['gridX']
            grid_y = point_data['properties']['gridY']

            # Get forecast data
            forecast_url = f"https://api.weather.gov/gridpoints/{grid_id}/{grid_x},{grid_y}"

            # Store forecast data
            forecast_response = requests.get(forecast_url, headers=headers, timeout=timeout)
            forecast_response.raise_for_status()
            forecast_data = forecast_response.json()

            return forecast_data

        except requests.exceptions.RequestException as err:
            retries += 1
            if retries >= max_retries:
                print(f"Error fetching weather data after {max_retries} attempts: {err}")
                return None
            else:
                # Exponential backoff
                wait_time = 2 ** retries
                print(f"Retry {retries}/{max_retries} after {wait_time}s: {err}")
                time.sleep(wait_time)

def extract_weather_features(noaa_data):
    """
    Extract relevant weather features from NOAA API response

    Args:
        noaa_data (dict): NOAA API response

    Returns:
        pd.DataFrame: DataFrame with weather features
    """
    if not noaa_data or "properties" not in noaa_data:
        return pd.DataFrame()

    records = []
    props = noaa_data["properties"]

    weather_properties = [
        "temperature",
        "dewpoint",
        "maxTemperature",
        "minTemperature",
        "relativeHumidity",
        "apparentTemperature",
        "heatIndex",
        "windChill",
        "wetBulbGlobeTemperature",
        "skyCover",
        "windDirection",
        "windSpeed",
        "windGust",
        "weather",
        "hazards",
        "probabilityOfPrecipitation",
        "quantitativePrecipitation",
        "iceAccumulation",
        "snowfallAmount",
        "snowLevel",
        "ceilingHeight",
        "visibility",
        "transportWindSpeed",
        "transportWindDirection",
        "mixingHeight",
        "hainesIndex",
        "lightningActivityLevel",
        "twentyFootWindSpeed",
        "twentyFootWindDirection",
        "waveHeight",
        "wavePeriod",
        "waveDirection",
        "primarySwellHeight",
        "primarySwellDirection",
        "secondarySwellHeight",
        "secondarySwellDirection",
        "wavePeriod2",
        "windWaveHeight",
        "dispersionIndex",
        "pressure",
        "probabilityOfTropicalStormWinds",
        "probabilityOfHurricaneWinds",
        "potentialOf15mphWinds",
        "potentialOf25mphWinds",
        "potentialOf35mphWinds",
        "potentialOf45mphWinds",
        "potentialOf20mphWindGusts",
        "potentialOf30mphWindGusts",
        "potentialOf40mphWindGusts",
        "potentialOf50mphWindGusts",
        "potentialOf60mphWindGusts",
        "grasslandFireDangerIndex",
        "probabilityOfThunder",
        "davisStabilityIndex",
        "atmosphericDispersionIndex",
        "lowVisibilityOccurrenceRiskIndex",
        "stability",
        "redFlagThreatIndex",
    ]
    # Extract time series data for each property
    for prop in weather_properties:
        if prop in props:
            for entry in props[prop]["values"]:
                valid_time = entry["validTime"]

                # https://www.w3.org/TR/NOTE-datetime
                # 1994-11-05T08:15:30-05:00
                # November 5, 1994, 8:15:30 am, US Eastern Standard Time.

                time_parts = valid_time.split("T")
                date_part = time_parts[0]
                time_part = (
                    time_parts[1].split("-")[0]
                    if "-" in time_parts[1]
                    else time_parts[1].split("+")[0]
                )

                # Combine date and time parts
                datetime_str = f"{date_part} {time_part}"

                # Convert to datetime
                try:
                    dt = datetime.strptime(datetime_str, "%Y-%m-%d %H:%M:%S")
                except ValueError:
                    # Try alternative format if the first one fails
                    dt = datetime.strptime(
                        f"{date_part} {time_part}", "%Y-%m-%d %H:%M:%S"
                    )

                # Create record
                record = {"timestamp": dt, "property": prop, "value": entry["value"]}
                records.append(record)

            # Create DataFrame
    df = pd.DataFrame(records)

    # Pivot to create wide format
    if not df.empty:
        df_pivot = df.pivot_table(
            index="timestamp", columns="property", values="value", aggfunc="first"
        ).reset_index()
        return df_pivot

        df_pivot = df_pivot.ffill().bfill()

    return pd.DataFrame()

def clean_weather_data(weather_df):
    """
    Clean NaN values in weather data

    Args:
        weather_df (pd.DataFrame): Weather data

    Returns:
        pd.DataFrame: Cleaned weather data
    """
    # Make a copy to avoid modifying the original
    df = weather_df.copy()

    # Fill NaN values using forward and backward fill
    df = df.ffill().bfill()

    # For any remaining NaNs, replace with appropriate values
    numeric_cols = df.select_dtypes(include=['float64', 'int64']).columns
    df[numeric_cols] = df[numeric_cols].fillna(0)

    categorical_cols = df.select_dtypes(include=['object']).columns
    df[categorical_cols] = df[categorical_cols].fillna('unknown')

    return df

############## PART 3: Data Integration ##############
def get_county_coordinates():
    """
    Calculate central coordinates for each county in the dataset

    Args:
        processed_df (pd.DataFrame): Processed outage data

    Returns:
        pd.DataFrame: Dataframe with county coordinates
    """

    BASE_PATH = "/content/gdrive/MyDrive/2025_GDG_Solutions/model/data/"
    COUNTY_COORD_PATH = os.path.join(BASE_PATH, "county_coordinates_tn.csv")
    # coordinates obtained from csv download
    # https://public.opendatasoft.com/explore/embed/dataset/us-county-boundaries/table/?flg=en-us&disjunctive.stusab&disjunctive.statefp&disjunctive.countyfp&disjunctive.name&disjunctive.namelsad disjunctive.state_name&sort=countyfp&refine.stusab=TN&location=8,35.61042,-86.35803&basemap=jawg.light&dataChart=eyJxdWVyaWVzIjpbeyJjb25maWciOnsiZGF0YXNldCI6InVzLWNvdW50eS1ib3VuZGFyaWVzIiwib3B0aW9ucyI6eyJmbGciOiJlbi11cyIsImRpc2p1bmN0aXZlLnN0dXNhYiI6dHJ1ZSwiZGlzanVuY3RpdmUuc3RhdGVmcCI6dHJ1ZSwiZGlzanVuY3RpdmUuY291bnR5ZnAiOnRydWUsImRpc2p1bmN0aXZlLm5hbWUiOnRydWUsImRpc2p1bmN0aXZlLm5hbWVsc2FkIjp0cnVlLCJkaXNqdW5jdGl2ZS5zdGF0ZV9uYW1lIjp0cnVlLCJzb3J0IjoiY291bnR5ZnAiLCJyZWZpbmUuc3R1c2FiIjoiVE4ifX0sImNoYXJ0cyI6W3siYWxpZ25Nb250aCI6dHJ1ZSwidHlwZSI6ImNvbHVtbiIsImZ1bmMiOiJBVkciLCJ5QXhpcyI6ImFsYW5kIiwic2NpZW50aWZpY0Rpc3BsYXkiOnRydWUsImNvbG9yIjoiI0ZGNTE1QSJ9XSwieEF4aXMiOiJzdGF0ZWZwIiwibWF4cG9pbnRzIjo1MCwic29ydCI6IiJ9XSwidGltZXNjYWxlIjoiIiwiZGlzcGxheUxlZ2VuZCI6dHJ1ZSwiYWxpZ25Nb250aCI6dHJ1ZX0%3D

    df = pd.read_csv(COUNTY_COORD_PATH)
    counties_df = df[['Geo Point', 'Geo Shape', 'GEOID', 'NAME', 'NAMELSAD', 'STUSAB', 'STATE_NAME']].copy()
    # rename columns for clarity
    new_cols = ['geo_point', 'geo_shape', 'fips', 'name', 'name_county', 'state_abbrev', 'state']
    counties_df.columns = new_cols

    # extract latitude and longitude from 'geo_point' column
    counties_df[['latitude', 'longitude']] = counties_df['geo_point'].str.split(',', expand=True).astype(float)

    # create a more user-friendly county name for use with NOAA API
    counties_df['county'] = counties_df['name']

    return counties_df

def fetch_weather_for_counties(county_coords_df, start_date, end_date):
    """
    Fetch weather data for all counties using the county coordinates DataFrame

    Args:
        county_coords_df (pd.DataFrame): County coordinates from get_county_coordinates()
        start_date (str): Start date in YYYY-MM-DD format
        end_date (str): End date in YYYY-MM-DD format

    Returns:
        dict: Dictionary with county name as key and weather data as value
    """
    county_weather = {}

    for _, row in county_coords_df.iterrows():
        print(f"Fetching weather data for {row['county']}...")
        county_name = row['county']

        # Fetch weather data using latitude and longitude
        weather_data = fetch_noaa_weather(
            row['latitude'],
            row['longitude'],
            start_date,
            end_date
        )

        if weather_data:
            # Extract features
            weather_df = extract_weather_features(weather_data)
            if not weather_df.empty:
                weather_df['county'] = county_name
                weather_df['fips'] = row['fips']  # Keep FIPS for mapping
                county_weather[county_name] = weather_df



    return county_weather

def combine_outage_and_weather_hybrid(historical_df, current_weather):
    """
    Match historical outages with current weather by season/conditions with improved error handling

    Args:
        historical_df (pd.DataFrame): Historical outage data
        current_weather (dict): Dictionary mapping county names to weather DataFrames

    Returns:
        pd.DataFrame: Combined outage and weather data
    """
    import pandas as pd
    import numpy as np
    from datetime import datetime

    # Return empty dataframe if inputs are empty
    if historical_df.empty or not current_weather:
        return pd.DataFrame()

    combined_data = []

    # Process each historical outage
    for idx, outage in historical_df.iterrows():
        # Skip if county is missing or not in weather data
        if 'county' not in outage or pd.isna(outage['county']):
            continue

        county = outage['county']

        # Safely handle county matching - convert to string and lowercase
        county_str = str(county).lower()

        # Find matching county in weather data keys
        matching_county = None
        for weather_county in current_weather.keys():
            if str(weather_county).lower() == county_str:
                matching_county = weather_county
                break

        # Skip if no matching county found
        if matching_county is None:
            continue

        # Get weather data for the matching county
        weather_df = current_weather[matching_county]

        # Skip if weather data is empty
        if weather_df.empty:
            continue

        # Get historical date info
        if not pd.api.types.is_datetime64_dtype(outage['start_time']):
            hist_date = pd.to_datetime(outage['start_time'])
        else:
            hist_date = outage['start_time']

        hist_month = hist_date.month
        hist_day = hist_date.day

        # Make sure timestamp column is datetime
        if 'timestamp' in weather_df.columns and not pd.api.types.is_datetime64_dtype(weather_df['timestamp']):
            weather_df['timestamp'] = pd.to_datetime(weather_df['timestamp'])

        # Skip if no timestamp column
        if 'timestamp' not in weather_df.columns:
            continue

        # Match by similar month (within 15 days)
        try:
            year = 2024
            if hist_month == 2 and hist_day > 29:
                hist_day = 29

            days_in_month = {
                1: 31, 2: 29, 3: 31, 4: 30, 5: 31, 6: 30,
                7: 31, 8: 31, 9: 30, 10: 31, 11: 30, 12: 31
            }

            if hist_day > days_in_month.get(hist_month, 30):
                hist_day = days_in_month.get(hist_month, 30)

            ref_date = pd.Timestamp(year=year, month=hist_month, day=hist_day)

            # Calculate day of year for each weather timestamp with explicit int conversion
            weather_df['dayofyear'] = weather_df['timestamp'].dt.dayofyear
            ref_dayofyear = ref_date.dayofyear


            matched_rows = []
            for idx, row in weather_df.iterrows():
                if abs(row['dayofyear'] - ref_dayofyear) <= 15:
                    matched_rows.append(idx)

            matched_weather = weather_df.loc[matched_rows].copy()

            # Clean up temporary column
            weather_df.drop('dayofyear', axis=1, inplace=True)
            if 'dayofyear' in matched_weather.columns:
                matched_weather.drop('dayofyear', axis=1, inplace=True)
        except Exception as e:
            print(f"Error matching dates: {e}")
            continue

        if not matched_weather.empty:
            # Take the median for each weather parameter
            weather_cols = [col for col in matched_weather.columns
                          if col not in ['timestamp', 'county', 'fips', 'dayofyear']]
            weather_features = {}

            for col in weather_cols:
                # Only include numeric columns
                if matched_weather[col].dtype in [np.float64, np.int64] and not matched_weather[col].isna().all():
                    try:
                        # Use median to avoid extreme values
                        weather_features[f'weather_{col}'] = matched_weather[col].median()
                    except Exception as e:
                        print(f"Error calculating median for {col}: {e}")

            # Combine with outage data
            outage_dict = outage.to_dict()

            # Check if any weather features were found
            if weather_features:
                combined_row = {**outage_dict, **weather_features}
                combined_data.append(combined_row)

    # Return a DataFrame or empty DataFrame
    if combined_data:
        return pd.DataFrame(combined_data)
    else:
        print("Warning: No matching data found after combining outage and weather.")
        return pd.DataFrame()

def select_priority_features(weather_df):
    # Define feature priority groups
    essential_features = [
        "temperature", "windSpeed", "probabilityOfPrecipitation",
        "quantitativePrecipitation", "relativeHumidity"
    ]

    high_priority = [
        "windGust", "dewpoint", "heatIndex", "skyCover",
        "hazards", "snowfallAmount", "visibility"
    ]

    medium_priority = [
        "apparentTemperature", "iceAccumulation", "transportWindSpeed",
        "mixingHeight", "windDirection"
    ]

    # Check available columns
    available_columns = [col for col in weather_df.columns
                         if any(feature in col for feature in
                               essential_features + high_priority + medium_priority)]

    # Select columns, adding 'timestamp', 'county', 'fips' if present
    base_cols = ['timestamp', 'county', 'fips']
    selected_cols = base_cols + available_columns

    # Only keep columns that actually exist
    existing_cols = [col for col in selected_cols if col in weather_df.columns]

    return weather_df[existing_cols]

def fetch_weather_alerts(county_fips, date=None):
    """
    Fetch weather alerts for a county using its FIPS code

    Args:
        county_fips (str): County FIPS code
        date (str, optional): Date of interest (not used for API call)

    Returns:
        dict: Alert data
    """
    headers = {
        'User-Agent': '(voltenn-project, your.email@example.com)',
        'Accept': 'application/geo+json',
    }

    # Create zone ID from FIPS (NWS uses format like 'TNC047')
    # Extract only the county part (last 3 digits)
    county_part = county_fips[-3:] if len(county_fips) >= 3 else county_fips
    zone_id = f"TNC{county_part}"

    # Call active alerts endpoint - no query parameters needed
    url = f"https://api.weather.gov/alerts/active/zone/{zone_id}"

    try:
        response = requests.get(url, headers=headers)
        response.raise_for_status()
        return response.json()
    except requests.exceptions.RequestException as e:
        print(f"Error fetching alerts: {e}")
        return None

def prepare_features_with_weather(combined_df):
    """
    Prepare features for the prediction model including weather data

    Args:
        combined_df (pd.DataFrame): Combined outage and weather data

    Returns:
        tuple: X (features) and y (labels) for model training
    """
    import pandas as pd
    import numpy as np
    from datetime import datetime, timedelta

    # Return empty data if input is empty
    if combined_df.empty:
        return pd.DataFrame(), pd.Series()

    # Create a copy to avoid modifying the original
    df = combined_df.copy()

    # Create binary target variable (1 = outage occurred)
    df['outage_occurred'] = 1

    # Create additional time-based features
    if 'start_time' in df.columns:
        if not pd.api.types.is_datetime64_dtype(df['start_time']):
            df['start_time'] = pd.to_datetime(df['start_time'], errors='coerce')

        # Only proceed if conversion was successful
        if not df['start_time'].isna().all():
            df['month'] = df['start_time'].dt.month
            df['day_of_week'] = df['start_time'].dt.dayofweek
            df['hour_of_day'] = df['start_time'].dt.hour

            # Add seasonal features
            df['sin_month'] = np.sin(2 * np.pi * df['month'] / 12)
            df['cos_month'] = np.cos(2 * np.pi * df['month'] / 12)

    # Create infrastructure age bins (if infrastructure_age exists)
    if 'infrastructure_age' in df.columns:
        # Handle potential non-numeric values by coercing to numeric
        df['infrastructure_age'] = pd.to_numeric(df['infrastructure_age'], errors='coerce')

        # Only create bins if there are valid numeric values
        if not df['infrastructure_age'].isna().all():
            df['age_bin'] = pd.cut(
                df['infrastructure_age'],
                bins=[0, 5, 10, 20, 30, 100],
                labels=['0-5', '6-10', '11-20', '21-30', '30+']
            )
            # One-hot encode age bins
            try:
                age_dummies = pd.get_dummies(df['age_bin'], prefix='age', dummy_na=False)
                df = pd.concat([df, age_dummies], axis=1)
            except Exception as e:
                print(f"Error creating age dummies: {e}")

    # One-hot encode categorical variables
    categorical_cols = [col for col in ['county', 'season', 'infrastructure_type',
                                     'infrastructure_condition']
                     if col in df.columns]

    for col in categorical_cols:
        try:
            df[col] = df[col].astype(str)
            dummies = pd.get_dummies(df[col], prefix=col, dummy_na=False)
            # Clean column names for compatibility
            dummies.columns = [col_name.replace(" ", "_").replace("-", "_").lower()
                            for col_name in dummies.columns]
            df = pd.concat([df, dummies], axis=1)
        except Exception as e:
            print(f"Error creating dummies for {col}: {e}")

    # Generate negative examples (non-outage events)
    non_outages = []

    for _, outage in df.iterrows():
        try:
            non_outage = outage.copy()

            # Shift time by a random amount (1-30 days)
            day_shift = np.random.randint(1, 30)
            shift_direction = 1 if np.random.random() > 0.5 else -1

            # Make sure we're working with datetime objects
            if 'start_time' in non_outage:
                if isinstance(non_outage['start_time'], str):
                    non_outage_time = datetime.strptime(non_outage['start_time'], '%Y-%m-%d %H:%M:%S')
                elif pd.isna(non_outage['start_time']):
                    # Skip if start_time is missing
                    continue
                else:
                    non_outage_time = non_outage['start_time']

                non_outage_time = non_outage_time + timedelta(days=day_shift * shift_direction)
                non_outage['start_time'] = non_outage_time

                # Update derived time features if they exist
                if 'month' in non_outage:
                    non_outage['month'] = non_outage_time.month
                if 'day_of_week' in non_outage:
                    non_outage['day_of_week'] = non_outage_time.weekday()
                if 'hour_of_day' in non_outage:
                    non_outage['hour_of_day'] = non_outage_time.hour
                if 'sin_month' in non_outage:
                    non_outage['sin_month'] = np.sin(2 * np.pi * non_outage_time.month / 12)
                if 'cos_month' in non_outage:
                    non_outage['cos_month'] = np.cos(2 * np.pi * non_outage_time.month / 12)

            # Modify weather values slightly to reflect non-outage conditions
            weather_cols = [col for col in non_outage.index if isinstance(col, str) and
                          col.startswith('weather_') and not isinstance(non_outage[col], pd.Series)]
            for col in weather_cols:
                try:
                    if pd.notna(non_outage[col]) and isinstance(non_outage[col], (int, float)):
                        # Decrease extreme values to make conditions less severe
                        if 'wind' in col or 'precip' in col:
                            non_outage[col] *= 0.7  # Reduce precipitation and wind by 30%
                except Exception as e:
                    print(f"Error modifying weather values for column {col}: {e}")

            # Set target to 0 (no outage)
            non_outage['outage_occurred'] = 0

            non_outages.append(non_outage)
        except Exception as e:
            print(f"Error generating non-outage example: {e}")
            continue

    # Combine outage and non-outage data
    if non_outages:
        try:
            all_data = pd.concat([df, pd.DataFrame(non_outages)], ignore_index=True)
        except Exception as e:
            print(f"Error combining outage and non-outage data: {e}")
            all_data = df
    else:
        all_data = df

    # Select features for model - ensure we're only using strings as column names
    weather_features = [col for col in all_data.columns
                      if isinstance(col, str) and col.startswith('weather_')]

    # Check if time features exist in the data
    valid_time_features = []
    for feature in ['month', 'day_of_week', 'hour_of_day', 'sin_month', 'cos_month']:
        if feature in all_data.columns:
            valid_time_features.append(feature)
    time_features = valid_time_features

    infra_features = [col for col in all_data.columns
                     if isinstance(col, str) and
                     (col.startswith('age_') or col.startswith('infrastructure_'))]

    county_features = [col for col in all_data.columns
                      if isinstance(col, str) and col.startswith('county_')]

    season_features = [col for col in all_data.columns
                      if isinstance(col, str) and col.startswith('season_')]

    # Combine all feature groups
    feature_cols = (weather_features + time_features + infra_features +
                  county_features + season_features)

    # Keep only columns that exist in the data
    feature_cols = [col for col in feature_cols if col in all_data.columns]

    # Create X and y for model training
    X = all_data[feature_cols].copy()
    y = all_data['outage_occurred'].copy()

    # Convert boolean columns to int
    for col in X.columns:
        if X[col].dtype == bool:
            X[col] = X[col].astype(int)

    # Handle any remaining NaN values
    X = X.fillna(0)
    # Then explicitly convert to appropriate types
    X = X.infer_objects(copy=False)  # Address the pandas FutureWarning
    
    if not X.empty:
        combined_df = X.copy()
        combined_df['outage_occurred'] = y
        combined_df.to_csv('bigquery_training_data.csv', index=False)
        print(f"Exported {len(combined_df)} records to bigquery_training_data.csv")

    return X, y





def build_and_train_model(X, y, test_size=0.2, validation_size=0.25):
    """
    Build and train a TensorFlow Decision Forests model with corrected implementation
    """
    import tensorflow as tf
    import tensorflow_decision_forests as tfdf
    import pandas as pd
    import numpy as np
    from sklearn.model_selection import train_test_split
    from sklearn.metrics import classification_report, confusion_matrix

    # Check if inputs are valid
    if X.empty or len(y) == 0:
        print("Error: Empty input data")
        return None, {"error": "Empty input data"}

    # Ensure all column names are strings
    X = X.copy()
    X.columns = [str(col) for col in X.columns]

    # Ensure there are no NaN values
    for col in X.columns:
        X[col] = np.where(pd.isna(X[col]), 0, X[col])

    # Convert any Series to strings where appropriate
    for col in X.columns:
        if isinstance(X[col].iloc[0], pd.Series):
            print(f"Converting Series column: {col}")
            X[col] = X[col].apply(lambda s: str(s) if isinstance(s, pd.Series) else s)

    # First split data
    try:
        X_train_val, X_test, y_train_val, y_test = train_test_split(
            X, y, test_size=test_size, random_state=42, stratify=y
        )

        X_train, X_val, y_train, y_val = train_test_split(
            X_train_val, y_train_val, test_size=validation_size, random_state=42, stratify=y_train_val
        )

        print(f"Training set size: {len(X_train)}")
        print(f"Validation set size: {len(X_val)}")
        print(f"Test set size: {len(X_test)}")
    except Exception as e:
        print(f"Error splitting data: {e}")
        return None, {"error": f"Error splitting data: {e}"}

    # Create TF Datasets using numpy arrays with larger batch size to avoid the warning
    try:
        # Convert X DataFrames to dictionaries of numpy arrays
        def df_to_dict(df):
            return {str(col): df[col].values for col in df.columns}

        train_features = df_to_dict(X_train)
        val_features = df_to_dict(X_val)
        test_features = df_to_dict(X_test)

        # Convert y Series to numpy arrays
        train_labels = y_train.values
        val_labels = y_val.values
        test_labels = y_test.values

        # Create datasets with larger batch size of 1000
        train_ds = tf.data.Dataset.from_tensor_slices((
            train_features, train_labels
        )).batch(1000)

        val_ds = tf.data.Dataset.from_tensor_slices((
            val_features, val_labels
        )).batch(1000)

        test_ds = tf.data.Dataset.from_tensor_slices((
            test_features, test_labels
        )).batch(1000)
    except Exception as e:
        print(f"Error creating TensorFlow datasets: {e}")
        return None, {"error": f"Dataset creation failed: {e}"}

    # Define model - fix the Sequential model creation issue
    try:
        print("Creating model...")
        # Don't use Sequential, use the GradientBoostedTreesModel directly
        model = tfdf.keras.GradientBoostedTreesModel(
            task=tfdf.keras.Task.CLASSIFICATION,
            verbose=2,
            num_trees=100,
            max_depth=6,
            min_examples=10
        )

        print("Compiling model...")
        model.compile(metrics=["accuracy"])
    except Exception as e:
        print(f"Error creating model: {e}")
        print("Attempting alternative model creation...")

        # Try alternative model creation with RandomForestModel
        try:
            model = tfdf.keras.RandomForestModel(
                task=tfdf.keras.Task.CLASSIFICATION,
                verbose=2
            )
            model.compile(metrics=["accuracy"])
        except Exception as e2:
            print(f"Alternative model creation also failed: {e2}")
            return None, {"error": f"Model creation failed: {e2}"}

    # Train model safely
    try:
        print("Training model...")
        history = model.fit(
            train_ds,
            validation_data=val_ds,
            verbose=1
        )
    except Exception as e:
        print(f"Error training model: {e}")
        return None, {"error": f"Training failed: {e}"}

    # Evaluate model
    evaluation_dict = {'history': history}
    try:
        print("Evaluating model...")
        test_results = model.evaluate(test_ds, return_dict=True)
        evaluation_dict['evaluation'] = test_results

        # Get predictions
        y_pred_proba = model.predict(test_ds)

        # Convert predictions to appropriate form
        if len(y_pred_proba.shape) > 1 and y_pred_proba.shape[1] > 1:
            # Multi-class case
            y_pred = np.argmax(y_pred_proba, axis=1)
        else:
            # Binary classification
            y_pred = (y_pred_proba > 0.5).astype(int).flatten()

        # Calculate metrics
        conf_matrix = confusion_matrix(y_test, y_pred)
        class_report = classification_report(y_test, y_pred, output_dict=True)

        evaluation_dict['confusion_matrix'] = conf_matrix
        evaluation_dict['classification_report'] = class_report
        evaluation_dict['y_test'] = y_test
        evaluation_dict['y_pred'] = y_pred

        # Try to get feature importance
        try:
            if hasattr(model, 'make_inspector'):
                inspector = model.make_inspector()
                if hasattr(inspector, 'variable_importances'):
                    importance = inspector.variable_importances()
                    evaluation_dict['feature_importance'] = importance
                else:
                    print("Model inspector does not provide variable importances")
            else:
                print("Model does not have inspection capabilities")
        except Exception as e:
            print(f"Warning: Could not calculate feature importance: {e}")
    except Exception as e:
        print(f"Error during evaluation: {e}")
        evaluation_dict['error'] = str(e)

    return model, evaluation_dict

# src/main.py
def main(start_date='2022-01-01', end_date='2022-12-31'):
    """
    Main execution function for combining real outage and weather data and training a model

    Args:
        start_date (str): Start date for data in YYYY-MM-DD format
        end_date (str): End date for data in YYYY-MM-DD format

    Returns:
        tuple: Trained model and evaluation results
    """
    import pandas as pd
    import numpy as np
    import os
    import logging
    from sklearn.metrics import classification_report

    # Set up logging
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
        handlers=[
            logging.FileHandler("voltenn_model.log"),
            logging.StreamHandler()
        ]
    )

    # Dictionary to collect results and potential error states
    results = {
        'model': None,
        'evaluation': {},  # Empty dict instead of None to avoid KeyError
        'errors': []
    }

    try:
        # 1. Load Tennessee outage data
        print("Loading Tennessee outage data...")
        tn_data = load_tennessee_data()
        processed_df = tn_data['processed_df']

        # 2. Filter data for the specified date range
        processed_df['start_time'] = pd.to_datetime(processed_df['start_time'])
        date_filtered_df = processed_df[
            (processed_df['start_time'] >= start_date) &
            (processed_df['start_time'] <= end_date)
        ]

        print(f"Filtered data to {len(date_filtered_df)} outages between {start_date} and {end_date}")

        # 3. Get county coordinates
        county_coords = get_county_coordinates()
        print(f"Found {len(county_coords)} counties with coordinate data")

        # 4. Fetch weather data for each county
        print("Fetching weather data for all counties...")
        county_weather = fetch_weather_for_counties(county_coords, start_date, end_date)
        print(f"Successfully fetched weather data for {len(county_weather)} counties")

        # 5. Clean and select priority features from weather data
        print("Cleaning and selecting priority weather features...")
        for county, weather_df in county_weather.items():
            # Clean NaN values
            weather_df = clean_weather_data(weather_df)
            # Select priority features
            weather_df = select_priority_features(weather_df)
            # Update the dictionary with cleaned data
            county_weather[county] = weather_df

        # 6. Combine outage and weather data
        print("Combining outage and weather data...")
        combined_df = combine_outage_and_weather_hybrid(date_filtered_df, county_weather)
        print(f"Created combined dataset with {len(combined_df)} records")

        if combined_df.empty:
            error_msg = "ERROR: No matching data found after combining outage and weather data."
            print(error_msg)
            results['errors'].append(error_msg)
            return None, results

        # 7. Add alert data if available
        try:
            print("Fetching weather alerts for counties...")
            alert_count = 0
            for idx, row in combined_df.iterrows():
                if 'fips' in row and pd.notna(row['fips']):
                    try:
                        fips = str(int(row['fips']))

                        # Get alerts for this county
                        alerts = fetch_weather_alerts(fips)

                        if alerts and 'features' in alerts:
                            # Count alerts by type
                            alert_types = [feature['properties']['event'] for feature in alerts['features']]
                            combined_df.at[idx, 'weather_alert_count'] = len(alert_types)

                            # Flag severe alerts
                            severe_types = ['Tornado', 'Severe Thunderstorm', 'Flood', 'Winter Storm']
                            combined_df.at[idx, 'weather_severe_alert'] = any(t in severe_types for t in alert_types)
                            alert_count += 1
                    except Exception as alert_err:
                        print(f"Error processing alert for FIPS {row.get('fips', 'unknown')}: {alert_err}")
                        continue

            print(f"Added alert data for {alert_count} records")
        except Exception as e:
            print(f"Error fetching alerts: {e}")
            print("Continuing without alert data...")
            results['errors'].append(f"Alert data error: {e}")

        # 8. Prepare features for modeling
        print("Preparing features for modeling...")
        X, y = prepare_features_with_weather(combined_df)
        print(f"Prepared features: {X.shape[0]} samples, {X.shape[1]} features")
            
        if not X.empty:
            bigquery_df = X.copy()
            bigquery_df['outage_occurred'] = y
            bigquery_df.to_csv('bigquery_final_data.csv', index=False)
            print(f"Exported final dataset with {len(bigquery_df)} records to bigquery_final_data.csv")
            
            

        # 9. Build and train model
        print("Building and training model...")
        model, evaluation = build_and_train_model(X, y)

        # Store results
        results['model'] = model
        if evaluation:  # Make sure evaluation is not None
            results['evaluation'] = evaluation

        # 10. Print evaluation results if available
        print("\nModel Evaluation:")
        if model is not None and 'evaluation' in evaluation and evaluation['evaluation'] is not None:
            # Safely access evaluation metrics
            eval_metrics = evaluation.get('evaluation', {})

            # Print metrics if they exist
            metrics = ['binary_accuracy', 'precision', 'recall', 'auc']
            for metric in metrics:
                if metric in eval_metrics:
                    print(f"{metric.capitalize()}: {eval_metrics[metric]:.4f}")
                else:
                    print(f"{metric.capitalize()}: Not available")

            # Print confusion matrix
            if 'confusion_matrix' in evaluation:
                print("\nConfusion Matrix:")
                print(evaluation['confusion_matrix'])

            # Print classification report
            if 'y_test' in evaluation and 'y_pred' in evaluation:
                print("\nClassification Report:")
                print(classification_report(evaluation['y_test'], evaluation['y_pred']))
        else:
            print("Evaluation metrics not available")

        # UNCOMMENT IF ITS YOUR FIRST TIME SAVING THE MODEL
        # if model is not None:
        #     try:
        #         save_path = save_model(model)
        #         print(f"Model saved to {save_path}")
        #     except Exception as e:
        #         print(f"Error saving model: {e}")
        #         results['errors'].append(f"Model saving error: {e}")

        # 12. Save feature importance information if available
        if model is not None and 'feature_importance' in evaluation and evaluation['feature_importance']:
            try:
                if 'MEAN_DECREASE_IN_ACCURACY' in evaluation['feature_importance']:
                    imp = evaluation['feature_importance']['MEAN_DECREASE_IN_ACCURACY'][0]
                    imp_df = pd.DataFrame({
                        'Feature': imp['features'],
                        'Importance': imp['importances']
                    })
                    imp_df = imp_df.sort_values('Importance', ascending=False)
                    imp_df.to_csv('feature_importance.csv', index=False)
                    print("Feature importance saved to feature_importance.csv")
            except Exception as e:
                print(f"Error saving feature importance: {e}")
                results['errors'].append(f"Feature importance error: {e}")

    except Exception as e:
        print(f"Unexpected error in main execution: {e}")
        results['errors'].append(f"Main execution error: {e}")

    return results['model'], results
    
main()